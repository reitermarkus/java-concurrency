% !TEX TS-program = xelatex
%
\documentclass{article}
\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{upgreek}
\usepackage{enumerate}
\usepackage[binary-units=true]{siunitx}
\usepackage{setspace}% http://ctan.org/pkg/setspace
\usepackage{ragged2e}

\title{Exercise Sheet 6 - Documentation}
\date{}
\author{DÃ¶tlinger Lukas, Kaltschmid Michael, Reiter Markus}

\begin{document}
  \RaggedRight

  \maketitle

  \section{Exercise 1}
    The task was to measure execution time of the provided program with different directories and furthermore write a concurrent version of the program.
    \\[3pt]
    Benchmarking turns out be rather difficult because there are many variables to consider. For example on any first run of the program you are likely to be IO bound rather then CPU bound which more or less negates the benefit of parallelism. The type of storage used is therefore a huge factor. HDDs skew the results more than SSDs in this regard.
    \\[3pt]
    So in order to get somewhat accurate results we decided to run the sequantial and parallel version of the program twice to ensure traverses beeing cached and therefore only focus on CPU results.
    \\[3pt]
    The results were mixed. We could measure speedups of up to 4 when running in parallel with 12 threads. However the average speedups was a lot slower. The reason for this lies in the implementation and data to traverse. The approach for the parallel programm was to partition all folders of the root in equally sized arrays of paths and subsequently compute the size of every file recursively in those arrays on every thread.
    \\[3pt]
    One can already guess that this approach has its flaws because depending on the folder structure some threads have more work than others. So in a perfect world where all root folders have the same folder structure with the same files a speedup of n where n is the amount of logical cores, could be possible.
    \\[3pt]
    Which brings us to the actual task for this week, beeing implementing a concurrent version of the program from last week by using the fork-join pattern. Fork-join by its design is much better suited for this task than my naive solution of partitioning folders ever was. Reason for that is with the concept of work stealing no thread is ever under-utilized, which was the main problem of my previous solution.
    \\[3pt]
    Benchmarks are straight forward and not that suprising. On my machine with 12 Threads I could perceive the most gains with up to 4 threads. After that it was more or less diminishing returns.
    \\[24pt]
    \begin{doublespacing}
      \begin{align*}
        \begin{tabular}{|*{3}{c|}}
          \hline
          \multicolumn{3}{|c|}{\bfseries Sequential} \\
          \hline
          $folder$ & $size$ & $time$ \\
          \hline
          Program Files (x86) & $\SI{642887.49}{\mega\byte}$ & $\SI{15729}{\milli\second}$ \\
          \hline
          Pictures & $\SI{982.78}{\mega\byte}$ & $\SI{23}{\milli\second}$ \\
          \hline
          Windows & $\SI{19149.31}{\mega\byte}$ & $\SI{6560}{\milli\second}$ \\
          \hline
          Downloads & $\SI{86459.19}{\mega\byte}$ & $\SI{1451}{\milli\second}$ \\
          \hline
        \end{tabular}
      \end{align*}
    \end{doublespacing}
    \begin{doublespacing}
      \begin{align*}
        \begin{tabular}{|*{3}{c|}}
          \hline
          \multicolumn{3}{|c|}{\bfseries Parallel} \\
          \hline
          $folder$ & $size$ & $time$ \\
          \hline
          Program Files (x86) & $\SI{642887.49}{\mega\byte}$ & $\SI{5666}{\milli\second}$ \\
          \hline
          Pictures & $\SI{982.78}{\mega\byte}$ & $\SI{5}{\milli\second}$ \\
          \hline
          Windows & $\SI{19149.31}{\mega\byte}$ & $\SI{3847}{\milli\second}$ \\
          \hline
          Downloads & $\SI{86459.19}{\mega\byte}$ & $\SI{1166}{\milli\second}$ \\
          \hline
        \end{tabular}
      \end{align*}
    \end{doublespacing}
    \begin{doublespacing}
      \begin{align*}
        \begin{tabular}{|*{6}{c|}}
          \hline
          \multicolumn{6}{|c|}{\bfseries Fork / Join} \\
          \hline
          $folder$ & $size$ & \multicolumn{4}{|c|}{$time$} \\
          \hline
          \multicolumn{2}{|c|}{Threads} & 1 & 2 & 4 & 8 \\
          \hline
          Program Files (x86) & $\SI{642887.49}{\mega\byte}$ & $\SI{15536}{\milli\second}$ & $\SI{8694}{\milli\second}$ & $\SI{5868}{\milli\second}$ & $\SI{4767}{\milli\second}$ \\
          \hline
          Pictures & $\SI{982.78}{\mega\byte}$ & $\SI{25}{\milli\second}$ & $\SI{14}{\milli\second}$ & $\SI{10}{\milli\second}$ & $\SI{9}{\milli\second}$ \\
          \hline
          Windows & $\SI{19149.31}{\mega\byte}$ & $\SI{6386}{\milli\second}$ & $\SI{3990}{\milli\second}$ & $\SI{2941}{\milli\second}$ & $\SI{3220}{\milli\second}$ \\
          \hline
          Downloads & $\SI{86459.19}{\mega\byte}$ & $\SI{1433}{\milli\second}$ & $\SI{818}{\milli\second}$ & $\SI{554}{\milli\second}$& $\SI{498}{\milli\second}$ \\
          \hline
        \end{tabular}
      \end{align*}
    \end{doublespacing}

  \section{Exercise 2}

  The goal of this exercise was to create a program which outputs the occurences of a specified word in all text files inside of a given directory. To implement this, we used Rust with the \texttt{rayon} crate, which provides a \texttt{join} method that can be used to implement a fork/join algorithm. Here, we used the same approach to traverse directories as in the first exercise. The \texttt{join\_rec} method checks if the currently matched \texttt{Path} is a file, if it is, the method \texttt{word\_occurrences} is called on it, otherwise it will recurse into the subdirectory.

  The method \texttt{word\_occurrences} takes a \texttt{Regex} and a \texttt{Path} as arguments. It then opens the path as a file and uses \texttt{BufReader::read\_to\_string} to read the contents of the file. The last step is to call \texttt{Regex::find\_iter} on the file contents, which returns a vector of matches, which we simply call \texttt{count()} on, to get the number of occurrences. To show the speedup using fork/join with various number of threads, I ran the program on a directory containing my Git repositories, which is $\SI{529}{\mega\byte}$ in size. I then searched for the word \textit{if}, which occurs quite often inside source code files.

  \begin{doublespacing}
    \begin{align*}
      \begin{tabular}{|*{2}{c|}}
        \hline
        Threads & Time \\
        \hline
        $1$ & $\SI{11.18}{\second}$ \\
        \hline
        $2$ & $\SI{1.21}{\second}$ \\
        \hline
        $4$ & $\SI{0.99}{\second}$ \\
        \hline
        $8$ & $\SI{0.98}{\second}$ \\
        \hline
      \end{tabular}
    \end{align*}
  \end{doublespacing}

  As you can see, there is significant speedup from a single thread, i.e. sequential execution, and two threads. There is another jump in speed going from two to four threads, after that, the speedup is not noticable anymore, at least not with a directory of this size.

  Therefore, I conducted another test with a directory which is $\SI{6.9}{\giga\byte}$ in size. This shows that there is still noticable speedup between four and eight threads.

  \begin{doublespacing}
    \begin{align*}
      \begin{tabular}{|*{2}{c|}}
        \hline
        Threads & Time \\
        \hline
        $4$ & $\SI{5.50}{\second}$ \\
        \hline
        $8$ & $\SI{4.69}{\second}$ \\
        \hline
      \end{tabular}
    \end{align*}
  \end{doublespacing}
\end{document}
